{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS431/631 Big Data Infrastructure\n",
    "### Winter 2018 - Assignment 5\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please edit this (text) cell to provide your name and UW student ID number!**\n",
    "* **Name:** _replace this with your name_\n",
    "* **ID:** _replace this with your UW student ID number_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Overview\n",
    "For this assignment, you will be using Python and Spark to perform spam detection.   You will need to perform two tasks.   The first is to build spam prediction models, using training data sets and stochastic gradient descent (SGD).   The second is to use these models to predict whether the documents in a test data set are spam.\n",
    "The stochastic gradient descent technique that you will be using is based on [a paper](http://arxiv.org/abs/1004.5168) by Cormack, Smucker and Clarke.\n",
    "\n",
    "#### Training a Spam Classification Models\n",
    "To build a spam classification model, you will start with a training data set.   Each instance in the training set represents a single document, and is labeled to indicate whether that document should be considered to be spam or ham.\n",
    "An instance looks like this:\n",
    "```\n",
    "clueweb09-en0094-20-13546 spam 387908 697162 426572 161118 688171 ...\n",
    "```\n",
    "The first field, `clueweb09-en0094-20-13546`, is the (unique) document name.   The second field is the label, indicating whether the document should be considered spam (as in this example) or ham.   The remaining fields are integers representing *features* present in the document.   In this case, the features are hashed byte 4-grams, represented as integers.   Each training data set is stored as a text file, with one instance per line.   The training files have been preloaded into the directory `/u/cs451/public_html/spam` for you.   They are:\n",
    "* `spam.train.group_x.txt`   (25 MB)\n",
    "* `spam.train.group_y.txt`   (20 MB)\n",
    "* `spam.train.britney.txt`   (766 MB)\n",
    "\n",
    "#### Question 1 ( 5/20 marks)\n",
    "\n",
    "Your first task is to write a sequential SGD model trainer in Python (no Spark).   For our purposes, a model associates a *weight* with each feature.   The model trainer decides what these weights should be, based on the training instances.  Since you are going to be writing a model trainer based on SGD, the trainer should behave like this:\n",
    "```\n",
    "for each training instance T\n",
    "   predict whether T is spam or ham using the weights of the current model\n",
    "   update the model weights by comparing T's predicted label with its actual label\n",
    "```\n",
    "Of course, the important part is how to update the model.\n",
    "\n",
    "In [the paper](http://arxiv.org/abs/1004.5168), the model is used to assign a \"spamminess\" score to a document.   Documents with positive spamminess are predicted to be spam.   Those with negative spamminess are predicted to be ham.  The spamminess of a document $D$ is simply the sum of the weights (from the model) of each of the document's features:\n",
    "\\begin{equation}\n",
    "spamminess(D) = \\sum_{f \\in D}{w(f)}\n",
    "\\end{equation}\n",
    "where $w(f)$ is the weight assocated with feature $f$.\n",
    "\n",
    "The Python module `spamminess.py` defines a function `spamminess(F,W)` which computes this quantity.   This function takes two arguments, `F` and `W`.  `F` is a list of features (integers) associated the document whose spamminess you want to compute, and `W` is a dictionary representing the current model.  `W` maps features ($f$) to their weights ($w(f)$) under the model.\n",
    "\n",
    "In the cell below, you will find partial pseudo-code that shows how to implement the SGD model trainer defined by Cormack, Smucker, and Clarke.   It reads the training instances one at a time from one of the training files, and uses them to adjust then model weights.   You job is to turn this pseudo-code into actual runnable Python code that can\n",
    "be used to learn a model from any one of the training files.   Once your code has finished reading and processing all of the instances from the input training file, your code should print the 5 features with the highest scores and the five features with the lowest scores in the model you have learned.   For each of these 10 features, print both the feature and its score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Your solution to Question 1 should go into this cell\n",
    "\n",
    "from spamminess import spamminess\n",
    "from math import exp\n",
    "\n",
    "# initial empty dictionary mapping features to real-valued weights.   This is the model\n",
    "w = {}\n",
    "\n",
    "# model update parameter, empirically chosen\n",
    "delta = 0.002\n",
    "\n",
    "# open one of the training files - in this case, group_x\n",
    "with open('/u/cs451/public_html/spam/spam.train.group_x.txt') as f:\n",
    "    for line in f:\n",
    "#       each line represents a document\n",
    "#       read and parse the line\n",
    "#       Let:\n",
    "#         t represent the spam/ham tag for this document\n",
    "#         F represent the list of features for this document\n",
    "\n",
    "#       find the spamminess of the current document using the current model:\n",
    "#       score = spamminess(F,w)\n",
    "\n",
    "#       then, update the model:\n",
    "#       prob = 1.0/(1+exp(-score))\n",
    "#       for each feature f in F:\n",
    "#           if t == 'spam':\n",
    "#               increase w(f) by (1.0-prob)*delta (or set w(f) to (1.0-prob)*delta if f is not in the dict yet)\n",
    "#           elif t == 'ham':\n",
    "#               decrease w(f) by prob*delta (or set w(f) to -prob*delta if f is not in the dict yet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 (5/20 marks)\n",
    "\n",
    "Next, you should try implementing a Spark version of the SGD model trainer.   Before you start, create a new folder\n",
    "called `models` inside your cs431 folder.   Your Spark implementation should read a training file, train the model, and then output the model to the models folder.   For each of the three training sets, you should produce a different model.  The model output file that you generate should list the weight associated with each feature, with one feature per line, like this:\n",
    "```\n",
    "(802123, 0.0009858585991850937)\n",
    "(438450, 4.267897922108138e-05)\n",
    "(271525, 0.0013133437007968654)\n",
    "(92853, 0.0004300009932503611)\n",
    "```\n",
    "\n",
    "Use Spark's `saveAsTextFile` action to output your model.   For example, if you are training a model for the group_x training set, use `saveAsTextFile(\"models/group_x_model\")`.   This will actually cause Spark to create folder called `group_x_model`.   In the folder, there will be files with names like `part-00000` that contain the actual output data.  When you use `saveAsTextFile`, Spark will generate one `part-xxxx` file for each partition of the RDD that you are writing out.   In this case, you should have only a single partition (for the reason described below), so there should be only one `part-xxxx` file.\n",
    "\n",
    "Training the SGD model is an inherently sequential task, since the training instances update the model one at a time, and each instance's spamminess is computed using the model produced by that instance's predecessors.   This means that the only part of the training that you can parallelize using Spark is the parsing of the input file.   Once the input is parsed, your Spark implementation will have to force all of the instances into a single partition, and then apply the training function to the entire partition.   To see whether you are getting sensible results, you can compare the model you learn with Spark to the one that you learned with your sequential Python program from Question 1.\n",
    "\n",
    "Remember that training should occur entirely in Spark.  The training instances should never come into your driver program.\n",
    "\n",
    "Put your solution for Question 2 into the cell below, and use it to generate models from all three of the training files, leaving the results in your models folder.   For this assignment, it is more natural to use Spark's original RDD interface, rather than the DataFrame interface (why?).   The Spark initialization code in the cell below assumes that you will be using the RDD interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/u/cs451/packages/spark\")\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "sc = SparkContext(appName=\"YourTest\", master=\"local[2]\")\n",
    "\n",
    "from spamminess import spamminess\n",
    "from math import exp\n",
    "\n",
    "#### Your Solution to Question 2 here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3 (5/20 marks)\n",
    "\n",
    "When you train a model using SGD, the model you get depends on the order in which you handle the training instances.  To see this in action, try using the Spark SGD trainer you implemented for Question 2 to train a model from the group_x training set, but with the instances processed in a different order.  \n",
    "\n",
    "To do this, modify your trainer from Question 2 so that it will randomly reorder the training instances before using them to update the model.   One way to shuffle the training instances is to assign a random sort key to each training instance as you read it from the input file, and then sort the instances using the random sort key.\n",
    "\n",
    "Be sure that Spark is doing the work of shuffling the training instances.   Do not load the training instances into your driver program and sort them there.\n",
    "\n",
    "Once you have implemented the shuffled trainer, train a model using shuffled group_x training instances, and compare the resulting model with group_x model you learned without shuffling.   It is up to you how to do this comparision.  At a minimum, compare features with the highest weights in each model to see if they are similar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Put your shuffling Spark SGD trainer in this cell.\n",
    "#  In addition, add one or more new cells (code and/or text) below this one to \n",
    "#  compare the shuffled and unshuffled models you produced for group_x\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 (5/20  marks)\n",
    "\n",
    "Last but not least, you should write a Spark program that can be used to classify documents as spam or ham, using the classification models you produced.\n",
    "\n",
    "The test data, i.e., the document instances that you should classifiy, is located in the file\n",
    "* `/u/cs451/public_html/spam/spam.test.qrels.txt`\n",
    "\n",
    "Each line in this file represents a document that needs to be classified as spam or ham.  The format of this file is identical to the format of the files that hold the training instances.\n",
    "\n",
    "Write a Spark program that will load a model (from a specified folder under `models`), classify all of the instances in `spam.test.qrels.txt` using that model, and then output the results as a text file.   The contents of the output file should look like this:\n",
    "```\n",
    "(clueweb09-en0000-00-00142,spam,2.601624279252943,spam)\n",
    "(clueweb09-en0000-00-01005,ham,2.5654162439491004,spam)\n",
    "(clueweb09-en0000-00-01382,ham,2.5893946346394188,spam)\n",
    "```\n",
    "Each line of the output represents one test instance.   The first two fields are the document ID and the test label.  These are just copied from the test data.   The third field is the spamminess score of the document, produced by the spamminess function using the model you are classifying with.   The fourth field is the spam/ham prediction made by the model.\n",
    "\n",
    "Of course, your spam/ham classifier must **not** use the test label from the input when making its prediction.  The test labels are the \"ground truth\" against which your predictions are being compared.   Using them to make predictions would defeat the whole purpose of model-based classification.\n",
    "\n",
    "Make sure that classification of the test instances is done by Spark, not by your driver program.  Do ***not*** load the test instances or classification results into your driver program.\n",
    "Unlike model training, classification is easily parallelizable, since each document is classified independently.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your model based classifier implementation into this cell\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have installed a program in the CS451 account that can be used to evaluate your classification results.  Given your ouput file, in the proper format, it will compute the area under the receiver operating curve (ROC).   This is a common way to characterize classifier error.    The lower this score, the better.   The evaluation program should produce one line of output, like this\n",
    "```\n",
    "1-ROCA%: 17.25\n",
    "```\n",
    "\n",
    "Use your classifier to classify the test instances using each of the three classification models that you produced, which should result in three different output files.   Then, in the cell below,\n",
    "use the evaluation program to evaluate each of the three results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Run the evaluation program like this, after first replacing \"output-file\"\n",
    "#  with the name of the file that holds your classifier's output\n",
    "#  Note the \"!\" character, which is important.   This is the escape character\n",
    "#  that tells the notebook to run an external program.\n",
    "\n",
    "!/u/cs451/bin/spam_eval.sh output-file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Don't forget to save your workbook!   When you are finished and you are ready to submit your assignment, download your notebook file (.ipynb) from the hub to your machine, and then follow the submission instructions in the assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
